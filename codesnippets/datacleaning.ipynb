{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Loading Raw Data</h1>\r\n",
    "<h4>Scope:</h4>\r\n",
    "<ul>\r\n",
    "<li>Reading CSV files</li>\r\n",
    "<li>Merging various datasets</li>\r\n",
    "<li>Deleting duplicate rows</li>\r\n",
    "<li>Deleting empty columns</li>\r\n",
    "<li>Maintaining relevant columns</li>\r\n",
    "<li>Shuffling data</li>\r\n",
    "<li>Reseting index of the data</li>\r\n",
    "</ul>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "#Loading CSV files\r\n",
    "first = pd.read_csv('afghanistan.csv',sep='\\t',engine='python',encoding='utf-16') #Setting the encoding to utf-16 prevent parseErrors\r\n",
    "second = pd.read_csv('Belarus.csv',sep='\\t',engine='python',encoding='utf-16') #The choice of python as the engine is based on preference, you can choose c\r\n",
    "third = pd.read_csv('Ethiopia1.csv',sep='\\t',engine='python',encoding='utf-16')# The csv files are tab separated hence /t\r\n",
    "fourth = pd.read_csv('Ethiopia2.csv',sep='\\t',engine='python',encoding='utf-16')\r\n",
    "\r\n",
    "# Concatenating all the loaded files\r\n",
    "merged_data = pd.concat([first,second,third,fourth]).\\\r\n",
    "        drop_duplicates(subset=['URL','Date','Hit Sentence']).\\\r\n",
    "            dropna(axis=1,how='all') # Deleting possible duplicate items and deleting columns with null values\r\n",
    "\r\n",
    "#Trimming the data to maintain potential useful variables and declutering\r\n",
    "#Variables such as Alternate Date Format and URL are maintain though they may have very low influence on the data\r\n",
    "trimmed_data = merged_data[['Alternate Date Format',\\\r\n",
    "                            'Twitter Followers','Twitter Following',\\\r\n",
    "                            'Reach','Country','Hit Sentence','URL']].\\\r\n",
    "                                sample(frac=1).\\\r\n",
    "                                    reset_index(drop=True) #shuffling data to mix theme and reseting index of dataframe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Deleting non-UTF-8 characters</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "trimmed_data['Hit Sentence'] = [bytes(tweet, 'utf-8').decode('utf-8', 'ignore') for tweet in trimmed_data['Hit Sentence']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Helper Class for further optional cleaning</h1>\r\n",
    "<h4>Scope</h4>\r\n",
    "<ul>\r\n",
    "<li>\r\n",
    "Standardizing twitter accounts mentioned in tweets\r\n",
    "</li>\r\n",
    "<li>\r\n",
    "Removing countries used in the data extraction\r\n",
    "</li>\r\n",
    "</ul>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "class firstProcess:\r\n",
    "\r\n",
    "    def mentionsStandardization(self,data:pd.DataFrame,tweet_column='Hit Sentence'):\r\n",
    "        \"\"\"[This method converts every tweet word token that begins with @w+ into mentions ]\r\n",
    "\r\n",
    "        Args:\r\n",
    "            data (pd.DataFrame): [dataframe object that contains tweets]\r\n",
    "            tweet_Column (str, optional): [The column that contains the tweets, default=Hit Sentence]\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            [pd.DataFrame]: Returns a dataframe object\r\n",
    "        \"\"\"\r\n",
    "        data[tweet_column] = list(map(self.mentionscapture,data[tweet_column]))\r\n",
    "        return data\r\n",
    "\r\n",
    "\r\n",
    "    def mentionscapture(self,tweet):\r\n",
    "        \"\"\"This method is reponsible for converting all mentions from tweets\r\n",
    "\r\n",
    "        Args:\r\n",
    "            tweet ([str]): [Tweets]\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            [str]: [Returns the refined tweet]\r\n",
    "        \"\"\"\r\n",
    "        if tweet:\r\n",
    "            #If tweet is not empty convert every @mention to a unique word which is less likely to appear in a normal tweet\r\n",
    "            qt_tweet = re.sub(\"QT \\@\\w+\",'QUOTE_TWEET',tweet) #Converting all quoted mentions\r\n",
    "            rt_tweet = re.sub(\"RT \\@\\w+\",\"RE_TWEET\",qt_tweet) # Converting all retweeted mentions\r\n",
    "            refined_tweet = re.sub(\"\\@\\w+\",\"_MENTIONS_\",rt_tweet) #Converting all mentions in the tweet\r\n",
    "            return refined_tweet\r\n",
    "        else:\r\n",
    "            return np.NaN\r\n",
    "\r\n",
    "    def removingBiasedCountries(self,tweet):\r\n",
    "        \"\"\"\r\n",
    "        This method is reponsible for removing some of the keywords used in extracting this dataset\r\n",
    "        NB: The dataset was generated by searching for every refugee mentions that are associated with Lithuania,Belarus,Afghanistan,Ethiopia,Sudan and Tigray.\r\n",
    "        As a result, these countries will be constant in every tweet which might affect the classifications.\r\n",
    "        This is an optional method.\r\n",
    "        \r\n",
    "\r\n",
    "        Args:\r\n",
    "            tweet ([str]): [tweets]\r\n",
    "        Returns:\r\n",
    "            [str]: [Returns the refined tweet]\r\n",
    "\r\n",
    "        Todo:\r\n",
    "            Possible to go further and strip all flags associated with this country\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        if tweet:\r\n",
    "            # Removing countries and it's hashtags considering various common typos\r\n",
    "            refined_tweet = re.sub('#?(Lithuania|Belarus|Afghanistan|Afganistan|Ethiopia|Sudan|Tigray)(\\w+)?',' ',tweet,re.IGNORECASE)\r\n",
    "            return refined_tweet\r\n",
    "        else:\r\n",
    "            return np.NaN\r\n",
    "\r\n",
    "    def mainCountriesRemove(self,data:pd.DataFrame,tweet_Column='Hit Sentence'):\r\n",
    "        \"\"\"[The main method for removing countries]\r\n",
    "\r\n",
    "        Args:\r\n",
    "            data (pd.DataFrame): [description]\r\n",
    "            tweet_Column (str, optional): [description]. Defaults to 'Hit Sentence'.\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        data[tweet_Column]=list(map(self.removingBiasedCountries,data[tweet_Column]))\r\n",
    "        return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "# This is without removing countries. \r\n",
    "# To remove countries, replicate this line and replace trimmed_data with the new variable and\r\n",
    "# call the method mainCountriesRemove from the firstprocess class\r\n",
    "fully_cleaned_data = firstProcess().mentionsStandardization(trimmed_data)\r\n",
    "fully_cleaned_data.to_excel('cleaned_data.xlsx',index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('data_analysis': conda)"
  },
  "interpreter": {
   "hash": "6d47d43fe5375968013aeb2d2652e50181d036f4d81d9ca01ef41efb642fb8d6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}